{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('train.csv', delimiter=\",\", skiprows=1)\n",
    "test_data = np.loadtxt('test.csv', delimiter=\",\", skiprows=1)\n",
    "x_train=train_data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_data[:,1:]\n",
    "x_test=test_data[:,1:]\n",
    "y_train=train_data[:,0]\n",
    "y_test=test_data[:,0]\n",
    "#Lets check that all the classes are exist in our data\n",
    "unique_values = np.unique(y_train)\n",
    "number_of_unique_values = len(unique_values)\n",
    "print(number_of_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(X, title, ax):\n",
    "    ax.imshow(X, cmap='gray')\n",
    "    ax.axis('off') # hide the axes ticks\n",
    "    ax.set_title(title, color= 'black', fontsize=25)\n",
    "\n",
    "def printExamples(x_train,y_train,numOfclass,numOfExamples=4):\n",
    "    fig, axes = plt.subplots(numOfclass,numOfExamples, figsize=(12,12))\n",
    "    for i in range(0,numOfclass):\n",
    "        idx = np.random.choice(np.where(y_train == i)[0],size=numOfExamples)\n",
    "        for j in range(numOfExamples):\n",
    "            ax = axes[i, j]\n",
    "            img = x_train[idx[j],:].reshape(28,28) \n",
    "            title = str(int(y_train[idx[j]])) \n",
    "            plot_image(img, title,ax)   \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "printExamples(x_train,y_train,number_of_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(x_train,y_train,val_pct=0.2):\n",
    "    X_train=x_train[:round((1-val_pct)*x_train.shape[0]),1:]\n",
    "    X_val=x_train[round((1-val_pct)*x_train.shape[0]):,1:]\n",
    "    Y_train=y_train[:round((1-val_pct)*y_train.shape[0])]\n",
    "    Y_val=y_train[round((1-val_pct)*y_train.shape[0]):]\n",
    "\n",
    "    return X_train,X_val,Y_train,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X,type='minMax'):\n",
    "    eps = 1e-8\n",
    "    if type!='minMax' and type!= 'Zscore':\n",
    "        print(\"Invalid type of normalization.Using MinMax as default type\\n\")\n",
    "        type ='minMax'\n",
    "    if type=='minMax':\n",
    "        # get min and max per feature (except the first one which corresponds to the bias term)\n",
    "        X_min = np.min(X[:, 1:], axis=0)\n",
    "        X_max = np.max(X[:, 1:], axis=0)\n",
    "        return (X[:, 1:] - X_min)/(X_max - X_min + eps)\n",
    "    else:\n",
    "        X_mean=np.mean(X[:, 1:], axis=0)\n",
    "        X_sd=np.std(X[:, 1:], axis=0)\n",
    "        return (X[:, 1:] - X_mean)/(X_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Shift z by subtracting its max value in each row for numerical stability\n",
    "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate the exponential of the shifted z\n",
    "    z_exp = np.exp(z_shifted)\n",
    "    \n",
    "    # Normalize the exponentials by the sum across classes (columns) for each sample (row)\n",
    "    softmax_output = z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "    # z_k = z - np.ones((z.shape[0], 1)) @ (np.max(z, axis=0).reshape(1, z.shape[1]))\n",
    "    # return np.exp(z_k) / np.sum(np.exp(z_k), axis=0).reshape(1, z_k.shape[1])\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(Y,numOfClass):\n",
    "    y_oneHot=np.zeros((Y.size,numOfClass))\n",
    "    y_oneHot[np.arange(Y.size), Y.astype(int)]= 1\n",
    "    \n",
    "    return y_oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def croosEntropy(softmax_ypred,y_oneHot_true):\n",
    "    eps = 1e-8\n",
    "    n=softmax_ypred.shape[0]\n",
    "    #return np.mean(-(np.log(softmax_ypred[(y_oneHot_true==1)]+eps)))\n",
    "    L=np.sum(np.log(softmax_ypred +eps )*y_oneHot_true)\n",
    "    return -L/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dw(softmax_z, y_true, x,reg_coeff,w):\n",
    "    n = x.shape[0]  # number of samples\n",
    "    dL = (1 / n) * x.T @ (softmax_z-y_true)+2*reg_coeff*w\n",
    "    return dL\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X,W,y=None):\n",
    "    N_test = X.shape[0]  # number of training samples\n",
    "    # matrix-vector multiplication\n",
    "    z = X @ W\n",
    "   \n",
    "\n",
    "    softmax_z = softmax(z)\n",
    "    prediction=np.argmax(softmax_z, axis=1)\n",
    "    if y is None:\n",
    "        return prediction\n",
    "    # calculate loss\n",
    "    test_loss = croosEntropy(softmax_z,y)\n",
    "    # calc. accuracy\n",
    "    labels_indices = np.argmax(y, axis=1)\n",
    "    prediction = np.array(prediction).reshape(-1)\n",
    "    accuracy = np.mean(labels_indices==prediction)\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logistic_classification(X_train,X_val,y_train,y_val,yhot_test,batch_size,eta,epochs,reg_coeff):\n",
    "    N = X_train.shape[0]  # number of training samples\n",
    "    d = X_train.shape[1]  # dimension = 784 + 1 (for the bias term)\n",
    "    W = np.random.normal(loc=0.0, scale=0.01, size=(d, 10))  # initalize weights iid from a gaussian with small noise\n",
    "    train_losses,train_accuracy, val_losses, val_accuracy = ([] for i in range(4))\n",
    "\n",
    "    # iterations over entire dataset\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        train_acc=0\n",
    "        print(epoch)\n",
    "        # batch iterations whithin each dataset iteration \n",
    "        for batch_idx, idx_start in enumerate(range(0, N, batch_size)):\n",
    "            idx_end = min(idx_start + batch_size, N)\n",
    "            X_batch = X_train[idx_start:idx_end, :]  # take all data in the current batch\n",
    "            y_batch = y_train[idx_start:idx_end,]  # take relevant labels\n",
    "\n",
    "            # matrix-vector multiplication\n",
    "            z = X_batch @ W  \n",
    "            # calc. probaility of y_j = 1 for each input (M,)\n",
    "            softmax_z = softmax(z)\n",
    "            pred=np.argmax(softmax_z, axis=1)\n",
    "            # calc. accuracy\n",
    "            labels_in = np.argmax(y_batch, axis=1)\n",
    "            prediction = np.array(pred).reshape(-1)\n",
    "            train_acc += np.mean(labels_in==pred)\n",
    "            # calculate loss\n",
    "            batch_loss = croosEntropy(softmax_z,y_batch)+ reg_coeff*(np.linalg.norm(W)**2)\n",
    "            loss += batch_loss\n",
    "            \n",
    "            # compute gradient of the loss w.r.t W\n",
    "            delta_W = dL_dw(softmax_z, y_batch, X_batch,reg_coeff,W)\n",
    "            \n",
    "            # update W\n",
    "            W = W - eta * delta_W\n",
    "        ##### validation ####\n",
    "        val_acc,val_loss = test(X_val,W, y_val)\n",
    "        \n",
    "        # save for plotting\n",
    "        train_losses.append(loss / batch_idx)\n",
    "        train_accuracy.append(train_acc/batch_idx)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracy.append(val_acc)\n",
    "\n",
    "   \n",
    "    return train_losses,train_accuracy, val_losses, val_accuracy,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train,x_val,y_train,y_val=train_val_split(x_train,y_train,0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:, 1:] = normalize(x_train)\n",
    "x_val[:, 1:] = normalize(x_val)\n",
    "x_test[:, 1:] = normalize(x_test)\n",
    "# add constant \"1\" for the bias term\n",
    "N_train = x_train.shape[0]  # number of training samples\n",
    "x_train = np.concatenate((np.ones((N_train, 1)), x_train.reshape(N_train, -1)), axis=1)\n",
    "N_test = x_test.shape[0]  # number of test samples\n",
    "x_test = np.concatenate((np.ones((N_test, 1)),x_test.reshape(N_test, -1)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_val = x_val.shape[0]  # number of training samples\n",
    "x_val = np.concatenate((np.ones((N_val, 1)), x_val.reshape(N_val, -1)), axis=1)\n",
    "yhot_train=oneHot(y_train,len(unique_values))\n",
    "yhot_val=oneHot(y_val,len(unique_values))\n",
    "yhot_test=oneHot(y_test,len(unique_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0:20,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # batch size\n",
    "eta = 0.005 # learning rate\n",
    "epochs = 40  # number of times to iterate over the entier dataset\n",
    "reg_coeff=0.005\n",
    "train_losses,train_accuracy, val_losses, val_accuracy,W=logistic_classification(x_train,x_val,yhot_train,yhot_val,yhot_test,batch_size,eta,\n",
    "                                                                                epochs,reg_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy on validation set\n",
    "steps = np.arange(epochs)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "#ax1.set_title('test loss: %.3f, test accuracy: %.3f' % (test_loss, test_acc))\n",
    "ax1.plot(steps, train_losses, label=\"train loss\", color='red')\n",
    "ax1.plot(steps, val_losses, label=\"val loss\", color='green')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('acccuracy')  # we already handled the x-label with ax1\n",
    "ax2.plot(steps, val_accuracy, label=\"val acc\", color='blue')\n",
    "ax2.plot(steps, train_accuracy, label=\"train acc\", color='yellow')\n",
    "\n",
    "fig.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_accuracy[-1])\n",
    "print(train_accuracy[-1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(x):\n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu_derive(x):\n",
    "    res=np.zeros((x.shape[0],x.shape[1]))\n",
    "    res[x>0]=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_NN(x,w1,w2,b1,b2,y=None):\n",
    "    N_test = x.shape[0]  # number of training samples\n",
    "    # matrix-vector multiplication\n",
    "    z1=(x @ w1) +b1\n",
    "    h=Relu(z1)\n",
    "    y_pred=softmax((h @ w2) +b2)\n",
    "    prediction=np.argmax(y_pred, axis=1)\n",
    "    if y is None:\n",
    "        return prediction\n",
    "    # calculate loss\n",
    "    test_loss = croosEntropy(y_pred,y)+ reg_coeff*(np.linalg.norm(w1)**2 + np.linalg.norm(w2)**2)\n",
    "    # calc. accuracy\n",
    "    labels_indices = np.argmax(y, axis=1)\n",
    "    prediction = np.array(prediction).reshape(-1)\n",
    "    accuracy = np.mean(labels_indices==prediction)\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_classification(x_train,x_val,y_train,y_val,batch_size,layer_size,eta, epochs,reg_coeff):\n",
    "    N = x_train.shape[0]\n",
    "    num_of_pixels=x_train.shape[1]\n",
    "    #unique_values = np.unique(y_train)\n",
    "    num_of_class = 10\n",
    "    w1=np.random.rand(num_of_pixels,layer_size)\n",
    "    w2=np.random.rand(layer_size,num_of_class)\n",
    "    b1=np.random.rand(1,layer_size)\n",
    "    b2=np.random.rand(1,num_of_class)\n",
    "\n",
    "    train_losses,train_accuracy, val_losses, val_accuracy = ([] for i in range(4))\n",
    "    # iterations over entire dataset\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        train_acc=0\n",
    "        print(epoch)\n",
    "        # batch iterations whithin each dataset iteration \n",
    "        for batch_idx, idx_start in enumerate(range(0, N, batch_size)):\n",
    "            idx_end = min(idx_start + batch_size, N)\n",
    "            x_batch = x_train[idx_start:idx_end, :]  # take all data in the current batch\n",
    "            y_batch = y_train[idx_start:idx_end,]  # take relevant labels\n",
    "            z1=(x_batch @ w1) +b1\n",
    "            h=Relu(z1)\n",
    "            y_pred=softmax((h @ w2) +b2)\n",
    "            pred=np.argmax(y_pred, axis=1)\n",
    "            # calc. accuracy\n",
    "            labels_in = np.argmax(y_batch, axis=1)\n",
    "            prediction = np.array(pred).reshape(-1)\n",
    "            train_acc += np.mean(labels_in==pred)\n",
    "            # calculate loss\n",
    "            batch_loss = croosEntropy(y_pred,y_batch)+ reg_coeff*(np.linalg.norm(w1)**2 + np.linalg.norm(w2)**2)\n",
    "            loss += batch_loss\n",
    "            \n",
    "            # Compute gradients\n",
    "            dLdz2 = y_pred-y_batch\n",
    "            batch_len = len(range(idx_start,idx_end))         \n",
    "            dLdb2 = (1. / batch_len) * np.ones((1,batch_len)) @ dLdz2\n",
    "            dLdw2 = (1. / batch_len) * h.T @dLdz2 \n",
    "\n",
    "            dLdz1 = (dLdz2 @ w2.T) * Relu_derive(z1) #dLdh=w2\n",
    "            \n",
    "            dLdw1 = (1. / batch_len) * x_batch.T @ dLdz1\n",
    "            dLdb1 = (1. / batch_len) * np.ones((1,batch_len)) @ dLdz1 \n",
    "            \n",
    "            w2 = w2 * (1-eta*reg_coeff) - eta * (dLdw2 + 2*w2*reg_coeff)\n",
    "            b2 = b2 - eta * dLdb2\n",
    "            w1 = w1 * (1-eta*reg_coeff) - eta * (dLdw1 + 2*reg_coeff * w1)\n",
    "            b1 = b1 - eta * dLdb1\n",
    "            \n",
    "        ##### validation ####\n",
    "        val_acc,val_loss = test_NN(x_val,w1,w2,b1,b2, y_val)\n",
    "        \n",
    "        # save for plotting\n",
    "        train_losses.append(loss / batch_idx)\n",
    "        train_accuracy.append(train_acc/batch_idx)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracy.append(val_acc)\n",
    "\n",
    "   \n",
    "    return train_losses,train_accuracy, val_losses, val_accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # batch size\n",
    "eta = 0.005 # learning rate\n",
    "epochs = 40  # number of times to iterate over the entier dataset\n",
    "reg_coeff=0.005\n",
    "layer_size=50\n",
    "train_losses,train_accuracy, val_losses, val_accuracy=NN_classification(x_train,x_val,yhot_train,yhot_val,batch_size,layer_size,eta,\n",
    "                                                                         epochs,reg_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy on validation set\n",
    "steps = np.arange(epochs)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.plot(steps, train_losses, label=\"train loss\", color='red')\n",
    "ax1.plot(steps, val_losses, label=\"val loss\", color='green')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('acccuracy')  # we already handled the x-label with ax1\n",
    "ax2.plot(steps, val_accuracy, label=\"val acc\", color='blue')\n",
    "ax2.plot(steps, train_accuracy, label=\"train acc\", color='yellow')\n",
    "\n",
    "fig.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8300892857142858\n",
      "0.8347415951359084\n"
     ]
    }
   ],
   "source": [
    "print(val_accuracy[-1])\n",
    "print(train_accuracy[-1])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
